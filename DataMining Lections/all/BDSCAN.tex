\newpage
\section{Алгоритмы кластеризации}

\subsection{Виды расстояний}
\subsubsection{Между объектами}
{\it Для начала давайте разберёмся, как лучше всего выбирать расстояние между объектами?}

Вариантов выбора способа задания расстояния огромное множество, но мы будем рассматривать только некоторые, наиболее популярные, из них.
\begin{enumerate}
\item Минковского
\[
d_r({\bf x, y}) = \left( \sum\limits_{j=1}^{N} |x_j - y_j|^r \right)^{\frac{1}{r}}
\]
\item Евклидово $(r = 2)$
\[
d_{E} ({\bf x, y}) = d_2({\bf x, y})
\]
\item Манхеттен $(r = 1)$ "--- расстояние, которое надо пройти пешеходу.
\[
d_{M} ({\bf x, y}) = d_1({\bf x, y})
\]
\item $r = \infty$
\[
d_{\infty} ({\bf x, y}) = \max\limits_{j} |x_j - y_j|
\]
\item Жаккар
\[
d_{J} ({\bf x, y}) = 1 - \frac{|x_j \cap y_j|}{|x_j \cup y_j|}
\]
\item Косинус
\[
d_{C} ({\bf x, y}) =\arccos\frac{{\bf xy}}{||{\bf x}|| \cdot ||{\bf y}||}
\]
\item Правки\\
$d_{e}$ "--- наименьшее количество удалений и вставок, приводящее {\bf x} к {\bf y};
\item Хэмминг\\
$d_{H}$ "--- количество различных компонент в {\bf x} и {\bf y}.

\end{enumerate}


\subsubsection{Между кластерами}
А теперь предположим, что мы уже разбили наши объекты, на какое-то количество кластеров и хотим определить, на каком расстоянии кластеры расположены друг относительно друга.
Для этого используются следующие способы задания расстояния:
\begin{enumerate}
\item Single-linkage
\[
d_{\min} (C_i,C_j) = \min\limits_{{\bf x} \in C_i, {\bf y} \in C_j} ||{\bf x} - {\bf y}||
\]
\item Complete-linkage
\[
d_{\max} (C_i,C_j) = \max\limits_{{\bf x} \in C_i, {\bf y} \in C_j} ||{\bf x} - {\bf y}||
\]
\item Average
\[
d_{avg} (C_i,C_j) = \frac{1}{n_j n_i} \sum\limits_{{\bf x} \in C_i}  \sum\limits_{{\bf y} \in C_j} ||{\bf x} - {\bf y}||
\]
\item Mean
\[
d_{mean} (C_i,C_j) = ||{\bf m_i} - {\bf m_j}||
\]
\end{enumerate}
\newpage
\input{all/ierarch.tex}
\newpage

\subsection{DBSCAN}
DBSCAN "--- {\it <<Density Based Spatial Clustering of Applications with Noise>>} (плотностный алгоритм для кластеризации пространственных данных с присутствием шума) впервые был предложен Мартином Эстер, Гансом-Питером Кригель и их коллегами в 1996 году как решение проблемы разбиения (изначально пространственных) данных на кластеры произвольной формы.

Основная {\bf идея} алгоритма заключается в том, что внутри каждого кластера наблюдается {\it типичная плотность точек} (объектов), которая заметно выше, чем плотность вне этого кластера. Также есть области {\it шума}, плотность которых згачительно меньше плотности любого из кластеров. Дабы отделить шум от основных класстеров задаётся некоторое значение, соответствующее минимальному пороговому значению точек в кластере, а~именно {\it Min\_Pts}

А теперь для удобства рассуждений введём несколько определений.

\begin{Def}{Плотность} 
"--- количество объектов внутри сферы заданного радиуса $\e$.
\end{Def}
\begin{Def}{Core-объект} \label{core}
"--- объект, плотность вокруг котрого больше, чем  Min\_Pts.
\end{Def}
\begin{Def}{Граничный объект} 
"--- объект, плотность вокруг котрого меньше, чем  Min\_Pts, однако он находится в непосредственной близости с Core-объектом.
\end{Def}
\begin{Def}{Шум} 
"--- объект, который не является ни core-объектом, ни граничным объектом. 
\end{Def}
\begin{Def}{Кластеры}
"--- участки высокой плотности, состоящие из core-объектов и граничных-объектов, разделенные участками низкой плотности.
\end{Def}

Таким образом, для реализации алгоритма нам понадобиться два параметра: $\e$ и {\it Min\_Pts}.

\subsubsection{Алгоритм}
На вход подаём множество объектов --- $X$, $\e$-радиус <<соседства>> и минимальное количество объектов в~кластере $Min\_Pts$. Рассмотрим алгоритм в виде псевдокода на языке $Python$:
\begin{lstlisting}
def DBSCAN(X, eps, Min_Pts):
    initialize NV = X # not visited objects
    for x in NV:
        remove(NV, x) # mark as visited
        nbr = neighbours(x, eps) # set of neighbours
        if nbr.size < Min_Pts:
            mark_as_noise(x)
        else:
            C = new_cluster()
            expand_cluster(x, nbr, C, eps, min_pts, NV)
    return C

def expand_cluster(x, nbr, C, eps, min_pts, NV):
    add(x, C)
    for x1 in nbr:
        if x1 in NV: # object not visited
            remove(NV, x1) # mark as visited
            nbr1 = neighbours(x1, eps)
            if nbr1.size >= min_pts:
                # join sets of neighbours
                merge(nbr, nbr_1)
        if x1 not in any cluster:
            add(x1, C)
\end{lstlisting}

\subsubsection{Плюсы и минусы}
\begin{itemize}
\item[$+$] не требуется заранее знать количество кластеров;
\item[$+$] кластеры могут быть произвольной формы. К тому же может случиться такое, что один кластер будет полностью лежать в другом, но они не будут соприкасаться;
\item[$+$] в данном алгоритме присутствует понятие шума, поэтому он устойчив к выбросам;
\item[$+$] для работы алгоритмы достаточно всего два параметра.
\end{itemize}
\begin{itemize}
\item[$-$] не является вполне детерминированным, так как если граничные точки равноудалены сразу от двух кластеров, то они могут быть быть интерпретированными по разному в зависимости от порядка обхода множества объектов;
\item[$-$] не работает при большой разности в плотностях кластеров, потому что нельзя подобрать комбинацию $\e$ и {\it Min\_Pts} соответствующим образом сразу для всех кластеров;
\item[$-$] если нет чёткого понимания природы данных, с которыми предстоит работать, то бывает трудно подобрать параметры $\e$ и {\it Min\_Pts}.
\end{itemize}

\subsubsection{Вычислительная сложность}
В общем случае алгоритм DBSCAN имеет квадратичную вычислительную сложность ({\bf $n^2$}) за счёт поиска $\e$-соседства. Однако, если для этой цели использовать специальную структуру данных "--- $R*Tree$, то в результате сложность поиска $\e$-соседей для одной точки – $O(\log n)$. Таким образом общая вычислительная сложность алгоритма DBSCAN составляет {\bf $O(n \log n)$}.

\subsubsection{Пример}
Зафиксируем параметры $\e = \frac{\sqrt{13}}{2} + \delta$ (расстояние между точками B1 и B2 + некое маленькое $\delta > 0$) и~$Min\_Pts = 3$. В случае~(\ref{DBSCAN1}) в качестве стартовой точки выберем B1, а в случае~(\ref{DBSCAN2}) "--- B2. Несложно заметить, что результаты полчатся разные. 
В первом случае B2 "--- граничная точка для кластера B, 
а во втором "--- шум.
\begin{figure}[H]
\centering
    \includegraphics[width=60mm]{images/DBSCAN1.png}
    \caption{Пример кластеризации методом DBSCAN с начальной точкой B1.}
    \label{DBSCAN1}
\end{figure}
\begin{figure}[H]
\centering
    \includegraphics[width=60mm]{images/DBSCAN2.png}
    \caption{Пример кластеризации методом DBSCAN с начальной точкой B2.}
    \label{DBSCAN2}
\end{figure}


\subsection{Оценка качества алгоритма кластеризации}
{\it Вот предположим, что у нас есть какие-то данные, есть готовая кластеризация, но нам этого мало. Мы~сели и ручками написали свой алгоритм кластеризации. Как можно оценить качество нашего алгоритма?}

Пусть нам заведомо дана обучающая выборка, для которой правильная кластеризация $C$ известна. С помощью выбранного алгоритма получена кластеризация K и проверим, насколько K совпадает с C. Для этого используем такие понятия, как {\it Adjusted Rand Index} (ARI) и {\it Mutual Information} (MI)
\[
RI = \frac{a+b}{C^N_2}, \qquad ARI = \frac{RI - E_{rdm}[RI]}{\max(RI) - E_{rdm}[RI]},
\]
где $a$ "--- количество пар объектов, попавших в один кластер и в C, и в K,\\
а $b$ "--- кол-во пар объектов, попавших в разные кластеры и в C, и в K.
\[
MI = \sum\limits_{c \in C}\sum\limits_{c \in K} p(c,k) \log \frac{p(c,k)}{p(k)p(c)}.
\]


{\it А что делать, если изначально <<правильной>> кластеризации попросту нет в наличии?!} {\it И как выбирать параметры нашей для нашей кластеризации?!}

\subsubsection{Критерий Silhouette (<<силуэт>>)}
Пусть дана кластеризация $N$ объектов в $K$ кластеров, и в каждом кластере $C_k$ находтся $N_k$ объектов. Предположим, что~наш объект i попал в $C_k$, тогда
\begin{itemize}
\item a(i) "--- среднее расстояние от i объекта до объектов из его же кластера $C_k$:
\[
a(i) = \frac{1}{N_{k} - 1} \sum\limits_{j \in C_k } d_k(i,j),
\]
где $d_k(i,j)$ "--- расстояние между $i$ и $j$ объектами; 
\item $b(i) = \min\limits_{j \not= k} b_{j}(i)$, где $b_{j}(i)$ "--- минимальное среднее расстояние от i объекта до объектов из другого кластера $C_j$ (то есть расстояние до соседнего кластера, куда бы он мог попасть, если бы не в этот кластер).
\end{itemize}
Таким образом
\[
silhouette(i) = S_i = \frac{b(i) - a(i)}{\max \left(a(i),b(i)\right)} 
\]
Заметим, что если наша кластеризация хорошая, то $b(i)$  всегда будет больше, чем $a(i)$, и $\max(a(i),b(i)) = b(i)$.
А оценкой качества нашего алгоритма будет являться средний {\it silhouette} для всех точек из множества {\bf X.} То есть
\[
SWC = \frac{1}{N} \sum\limits_{j = 1}^{N} S_j.
\]
То значение $K$, для которого $SWC$ будет максимально, выбирается в качестве оптимального количества кластеров.
\begin{Zam}
Если $K$ окажется равным $1$, то $SWC = -1$.
\end{Zam}
\input{all/OPTICS.tex}
\input{all/BIRCH.tex}